# Persistence

kafka는 메시지를 저장하고 캐싱하는데 파일 시스템에 많이 의존한다. "디스크가 단지 느리다" 라고 대부분의 사람들은 생각할건데 사실 디스크는 사용 방법에 따라 예상보다 느릴 수도 있고 빠를 수도 있다. 디스크 성능에 대한 핵심적인 사실은 하드 드라이브의 처리량이 지난 10년간 디스크 검색 지연시간에서 벗어나고 있다는 것이다. 랜덤으로 읽고 쓰는 것 말고, 한 줄로 읽고 쓰는 것은 모든 패턴 중에서 예측이 가능하며, 운영체제에 의해 크게 최적화된다. 그리고 랜덤으로 읽고 쓰는 것 보다 한 줄로 읽고 쓰는데 걸리는 것이 확실히 빠르다는 연구 결과도 있다.

이러한 성능 차이를 보완하기 위해 요즘 운영 체제는 디스크 캐싱을 위해 메모리를 사용하는 방식을 사용하려고 한다. 최신 OS는 메모리 재확보 시 성능 저하 없이 사용 가능한 모든 메모리를 디스크 캐싱으로 전환해준다. 

더 나아가, Kafka는 JVM 기반으로 구축 중이다. Java 메모리를 사용하는 개발자들은 JVM에서 돌아가는 애플리케이션의 2가지 특징을 알고 있다.

* 객체의 메모리 오버헤드가 매우 높다. 보통 데이터가 저장되는 사이즈의 두배정도?
* 자바의 garbage collection은 힙 사이즈가 늘어남에 따라 더 성능이 저하된다.

이러한 특징 때문에 파일시스템을 사용하고 페이지 캐시에 의존하는 것이 메모리 내 캐시나 기타 구조를 유지하는 것보다 우수하다. 사용 가능한 모든 메모리에 자공으로 액세스하여 사용 가능한 캐시를 두 배 이상 늘리고, 개별 객체보다는 compact byte 구조를 저장함으로써 다시 두배로 늘릴 수도 있다. 이렇게 하면 GC의 패널티 없이 32GB 기계에 최대 28~30GB의 캐시가 생성된다. 게다가 이 캐시는 서비스가 재시작되더라도 warm 상태를 유지하는 반면, 프로세스 중인 캐시는 메모리에 재구축해야 하거나 완전 cold 캐시로 시작해야 한다. 이는 또한 캐시와 파일 시스템 간의 일관성을 유지하기 위한 모든 논리가 OS에 있기 때문에 코드를 크게 단순화한다. 디스크 사용이 선형 읽기를 선호하는 경우, 미리 읽기는 캐시를 각 디스크 읽기마다 효율적인 데이터로 미리 채우는 것이다. (hit 율이 높도록?)

이는 매우 간단한 설계를 제안한다. 공간이 부족할 때 가능한 많은 메모리를 유지하고, 한꺼번에 파일시스템에 보내는 것 보다 뒤집는 방식으로 한다. 모든 데이터는 디스크에 플러시하지 않고 파일 시스템에 로그로 즉시 작성된다. 이거는 단순히 커널의 pagecache로 전달된다.



## Constant Time Suffices

메시징 시스템에서 사용되는 영구 데이터 구조는 보통 컨슈머당 메시지와 관련된 메타데이터를 유지하기 위해 BTree와 관련되거나 다른 목적의 random access data 구조의 큐로 구성된다. BTree는 가장 유용한 자료구조이고 메시징 시스템에서 다양한 트랜잭션 및 비 트랜잭션을 지원한다. 대신 BTree는 다음의 경우엔 비용이 비싸다. BTree 연산은 O(log N)이다. 보통 O(log N)은 상수시간이랑 비슷하다. 그러나 Disk 연산에 대해서는 조금 다르다. Disk 탐색은 한 pop에 10ms이고, 각 디스크는 한 번에 한 번만 검색할 수 있으므로 병렬처리가 제한된다. 따라서 소수의 디스크도 매우 높은 오버헤드로 이어진다. 스토리지 시스템이 아주 느린 물리 디스크 연산과 아주 빠른 캐시 연산과 섞인다면 고정된 캐시에 따라 데이터가 증가하므로 트리구조의 체감되는 성능은 선형 이상이다.

직관적으로 영구적인 큐는 로깅과 같은 경우 처럼 파일에 간단히 읽고 쓰기 위해 구축된다. 이 구조는 평균 연산 속도가 O(1)이고 읽기나 쓰기는 서로를 block시키기 않는다는 장점이 있다. 이는 성능이 데이터 크기에 영향이 없기 때문에 명백한 성능적인 이점이 있다. 하나의 서버는 저렴한 비용으로도 최대한 활용할 수 있다.

어떠한 성능적인 패널티 없이 가상 디스크 공간에 접근하는 것은 메시징 시스템에서는 일반적으로 찾을 수 없는 기능을 제공할 수 있다. 예를 들어, 카프카에서는 메시지를 소비함과 동시에 삭제하는 것 대신 메시지를 소비해도 장기적으로 메시지를 유지할 수 있다. 이는 컨슈머를 위한 엄청난 유연성으로 이어진다.



# Efficiency

카프카는 효율성 높이기 위해 신경썼다. 주로 신경썼던 것 중에 하나가 웹 액티비티 데이터를 다루는 것이다. 웹 액티비티 데이터는 매우 큰 데이터이기 때문이다. 각 페이지 뷰는 수십개의 쓰기를 생성할 수 있다. 게다가 각각 출판된 메시지는 하나 이상의 컨슈머가 읽는다. 그래서 가능한 소비(consumption)를 매우 효율적으로 하려고 노력했다.

그리고 효율성은 효율적인 Multi-tenant 운영이 핵심이라는 것도 발견했다. 만약 애플리케이션의 사용 빈도가 낮아 다운스트림 인프라 서비스가 쉽게 병목 현상이 될 수 있다면 병목 현상과 같은 변화가 문제가 될 수도 있다. 애플리케이션이 처리하는 속도가 매우 빠르기 때문에, 애플리케이션이 인프라보다 먼저 오버헤드로 전환될 수 있다. 이는 사용 패턴의 변화가 거의 매일 발생하기 때문에 중앙화된 클러스터에서 수 십 - 수 백개의 애플리케이션을 지원하는 중앙 서비스를 실행하려 할 때 특히 중요하다.

이전 섹션에서 디스크 효율에 대해 이야기 하였다. 불량한 디스크 접근 패턴이 제거되면, 두 가지 일반적인 비효율성의 원인이 있다. (너무 많은 I/O 연산과 과도한 바이트 복사가 있다.) 이 작은 I/O 문제는 클라이언트와 서버 사이에서 발생할 수 있으며 서버의 persistent 연산에 의해 발생한다.

이런 문제를 피하기 위해서는 프로토콜은 자연스럽게 메시지를 그룹화하는 메시지 세트 추상화를 중심으로 구축된다. 이는 네트워크 요청이 한 번에 하나의 메시지를 보내는 대신 메시지를 그룹화하여 네트워크 왕복에 대한 오버헤드를 줄일 수 있다. 서버는 한 번에 메시지 청크를 로그에 추가하고 컨슈머는 큰 라인의 청크를 한번에 받는다. 

이 간단한 최적화는 엄청난 속도를 만들어낸다. 배치는 더 큰 네트워크 패킷과 시퀀스 디스크 연산과 근접한 메모리 블럭 등을 처리한다. 이 모든 것들은 카프카를 통해 컨슈머에게 들어가는 많은 랜덤 메시지 입력 스트림을 선형 입력스트림으로 전환할 수 있다.

다른 비효율성은 바이트 복사이다. 메시지 전송율이 적을 땐 큰 이슈가 되지는 않지만, 부하가 많아지면 영향이 크다. 이런 문제를 피하기 위해서 카프카는 프로듀서, 브로커, 컨슈머 사이에 공유되는 표준화된 이진 메시지 포멧을 사용한다. 

브로커에 의해 유지되는 메시지 로그는 그 자체가 파일의 디렉토리이며. 각각은 컨슈머와 프로듀서가 사용한 포멧과 같은 포멧으로 디스크에 작성된 일련의 메시지 셋에 의해 채워진다. 이러한 일반적인 포멧을 유지하는 것은 가장 중요한 연산인 영구 로그 청크를 네트워크로 전송하는 연산을 최적화하도록 한다. 현대 유닉스 연산 시스템은 페이지캐시에서 소켓으로 데이터를 전송하기 위해 고도로 최적화된 코드 경로를 제공한다. 리눅스에서는 이런 역할을 하는 것은 sendfile system call이다.

송신 파일의 영향을 이해하기 위해서는 파일에서 소켓으로 데이터를 전송하기 위한 공통 데이터 경로를 이해하는 것이 중요하다.

1. OS는 커널 공간에서 데이터를 디스크에서 페이지캐시로 읽는다.
2. 애플리케이션은 커널 공간에서 user-space buffer로 데이터를 읽는다.
3. 애플리케이션은 데이터를 다시 커널 공간의 소켓 버퍼에 쓴다.
4. OS는 데이터를 네트워크로 전송할 때 소켓 버퍼에서 NIC 버퍼로 복사한다. 

위와 같은 방식은 비효율적이다. 여기서는 4번의 복사와 두 번의 시스템 콜이 사용된다. sendfile을 사용하면 OS가 페이지 캐시에서 네트워크로 직접 데이터를 보낼 수 있으므로 재 복사가 방지된다. 그래서 최적화된 경로로 오직 NIC 버퍼로 단 한번의 복사로만 해결할 수 있다.

일반적인 유즈케이스는 한 토픽에 여러 컨슈머가 있을 것으로 생각한다. zero-copy 최적화를 사용한다면 데이터는 페이지캐시로 정확히 한번 복사되고 메모리에 저장되는 대신 각 소비에 재 사용되며 읽을 때 마다 user-space에 복사된다. 이렇게 하면 네트워크 연결 제한에 근접한 속도로 메시지를 사용할 수 있다.

페이지캐시와 sendfile의 조합은 kafka 클러스터에서 컨슈머는 캐시에서 데이터를 처리하기 때문에 디스크에서 읽기 작업이 수행되지 않는다.



## End-to-end Batch Compression

몇 가지 경우에서 병목현상은 CPU나 디스크가 아닌 네트워크 때문에 발생한다. 이는 특히 광역 네트워크를 통해 데이터 센터 간에 메시지를 전송해야 하는 데이터 파이프라인에 해당된다. 사용자는 항상 카프카로부터의 지원 없이 한 번에 하나씩 메시지를 압축할 수 있지만 이는 같은 타입의 메시지 사이의 반복으로 인해 매우 낮은 비율로 압축이 될 수 있다. 효율적인 압축은 각각의 메시지를 따로 압축하는 것 보다 다양한 메시지를 함께 압축하는 것이 필요하다.

Kafka는 압축을 효율적인 배치 포멧으로 지원한다. 메시지 일괄 처리는 메시지를 함께 압축할 수 있고 압축된 형태로 서버에 전송될 수 있다. 이러한 메시지 일괄 처리는 압축된 형태로 작성될 것이고 압축된 형태의 로그로 남으며 컨슈머에 의해서만 압축을 풀 수 있다.

Kafka는 GZIP, Snappy, LZ4, ZStandard 압축 프로토콜을 지원한다.







